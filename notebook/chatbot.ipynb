{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6757dada",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79424214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import psycopg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e86808c",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df37c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mod√®le d'embedding : nomic-embed-text\n",
      "‚úì Mod√®le de chat : mistral:latest\n",
      "‚úì Base de donn√©es : PostgreSQL Docker (port 5433)\n"
     ]
    }
   ],
   "source": [
    "db_connection_str = \"postgresql://postgres:postgres@localhost:5433/ai\"\n",
    "\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"  # Mod√®le pour les embeddings\n",
    "CHAT_MODEL = \"mistral:latest\"       # Mod√®le pour la g√©n√©ration de r√©ponses\n",
    "\n",
    "print(f\"‚úì Mod√®le d'embedding : {EMBEDDING_MODEL}\")\n",
    "print(f\"‚úì Mod√®le de chat : {CHAT_MODEL}\")\n",
    "print(f\"‚úì Base de donn√©es : PostgreSQL Docker (port 5433)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98c4b1",
   "metadata": {},
   "source": [
    "Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966b3940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fonctions d√©finies\n"
     ]
    }
   ],
   "source": [
    "def embed_text(text: str, model_name: str = EMBEDDING_MODEL) -> list[float]:\n",
    "    \"\"\"\n",
    "    G√©n√®re un embedding pour le texte donn√© avec Ollama\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        raise ValueError(\"Le texte ne peut pas √™tre vide\")\n",
    "    \n",
    "    response = ollama.embeddings(\n",
    "        model=model_name,\n",
    "        prompt=text\n",
    "    )\n",
    "    return response[\"embedding\"]\n",
    "\n",
    "def fetch_similar_from_db(query_embedding: list[float], top_k: int = 3, connection: str = db_connection_str):\n",
    "    \"\"\"\n",
    "    R√©cup√®re les top-k documents les plus similaires depuis la base de donn√©es\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples (id, corpus, similarity)\n",
    "    \"\"\"\n",
    "    if not query_embedding:\n",
    "        return []\n",
    "    \n",
    "    with psycopg.connect(connection) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                SELECT id, corpus,\n",
    "                       1 - (embedding <=> %s::vector) AS similarity\n",
    "                FROM embeddings\n",
    "                ORDER BY embedding <=> %s::vector\n",
    "                LIMIT %s\n",
    "                \"\"\",\n",
    "                (query_embedding, query_embedding, top_k),\n",
    "            )\n",
    "            return cur.fetchall()\n",
    "\n",
    "\n",
    "def build_prompt(question: str, docs: list[tuple], max_chars: int = 2000) -> str:\n",
    "    \"\"\"\n",
    "    Construit un prompt RAG avec le contexte et la question\n",
    "    \n",
    "    Args:\n",
    "        question: Question de l'utilisateur\n",
    "        docs: Liste de tuples (id, corpus, similarity)\n",
    "        max_chars: Limite de caract√®res pour le contexte\n",
    "    \n",
    "    Returns:\n",
    "        Prompt format√© pour le mod√®le\n",
    "    \"\"\"\n",
    "    context_parts = []\n",
    "    used = 0\n",
    "    \n",
    "    for _id, content, _sim in docs:\n",
    "        if not content:\n",
    "            continue\n",
    "        part = content.strip()\n",
    "        if used + len(part) > max_chars:\n",
    "            part = part[: max(0, max_chars - used)]\n",
    "        context_parts.append(part)\n",
    "        used += len(part)\n",
    "        if used >= max_chars:\n",
    "            break\n",
    "    \n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    \n",
    "    prompt = (\n",
    "        \"Tu es un assistant serviable. Utilise UNIQUEMENT les documents fournis ci-dessous pour r√©pondre √† la question.\\n\\n\"\n",
    "        \"Si la r√©ponse n'est pas dans les documents, dis 'Je ne sais pas'. Ne fabrique JAMAIS d'informations.\\n\\n\"\n",
    "        \"CONTEXTE:\\n\" + context_text + \"\\n\\n\"\n",
    "        \"QUESTION: \" + question + \"\\n\\n\"\n",
    "        \"R√©ponds de mani√®re claire et concise en te basant UNIQUEMENT sur le contexte fourni:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_answer(question: str, top_k: int = 3, chat_model: str = CHAT_MODEL, verbose: bool = False):\n",
    "    \"\"\"\n",
    "    Fonction principale : g√©n√®re une r√©ponse RAG compl√®te\n",
    "    \n",
    "    Args:\n",
    "        question: Question de l'utilisateur\n",
    "        top_k: Nombre de documents √† r√©cup√©rer\n",
    "        chat_model: Mod√®le Ollama √† utiliser\n",
    "        verbose: Afficher les d√©tails du prompt\n",
    "    \n",
    "    Returns:\n",
    "        dict avec 'answer', 'contexts' et 'prompt'\n",
    "    \"\"\"\n",
    "    # 1. Calculer l'embedding de la question\n",
    "    print(f\"üîç Recherche de documents similaires...\")\n",
    "    q_emb = embed_text(question)\n",
    "    \n",
    "    # 2. R√©cup√©rer les documents similaires\n",
    "    docs = fetch_similar_from_db(q_emb, top_k=top_k)\n",
    "    \n",
    "    if not docs:\n",
    "        return {\n",
    "            \"answer\": \"Aucun document pertinent trouv√© dans la base de donn√©es.\",\n",
    "            \"contexts\": [],\n",
    "            \"prompt\": \"\"\n",
    "        }\n",
    "    \n",
    "    print(f\"‚úì {len(docs)} document(s) trouv√©(s)\")\n",
    "    \n",
    "    # 3. Construire le prompt\n",
    "    prompt = build_prompt(question, docs)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nüìù Prompt g√©n√©r√© ({len(prompt)} caract√®res):\\n\")\n",
    "        print(\"=\" * 70)\n",
    "        print(prompt)\n",
    "        print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    # 4. G√©n√©rer la r√©ponse avec Ollama\n",
    "    print(f\"ü§ñ G√©n√©ration de la r√©ponse avec {chat_model}...\")\n",
    "    \n",
    "    response = ollama.generate(\n",
    "        model=chat_model,\n",
    "        prompt=prompt,\n",
    "        options={\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.9,\n",
    "            'num_predict': 500,  # Nombre maximum de tokens √† g√©n√©rer\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    answer = response['response']\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": docs,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    "\n",
    "print(\"‚úì Fonctions d√©finies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7530ee",
   "metadata": {},
   "source": [
    "Test avec une question simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9079268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : Quels sont les stages disponibles ?\n",
      "\n",
      "üîç Recherche de documents similaires...\n",
      "‚úì 3 document(s) trouv√©(s)\n",
      "ü§ñ G√©n√©ration de la r√©ponse avec mistral:latest...\n",
      "\n",
      "======================================================================\n",
      "üìù R√âPONSE\n",
      "======================================================================\n",
      " Les documents fournis ne contiennent pas d'informations sur les stages disponibles.\n",
      "\n",
      "======================================================================\n",
      "üìö SOURCES UTILIS√âES\n",
      "======================================================================\n",
      "\n",
      "[1] ID:205 | Similarit√©: 0.6572\n",
      "    h: sur le programme de demain matin\n",
      "\n",
      "[2] ID:163 | Similarit√©: 0.6425\n",
      "    h: et puis e dans un deuxi√®me temps # il faut que e j'appelle e mon coll√®gue de Lorient pour qu'il me r√©explique comment mettre en route le beans\n",
      "\n",
      "[3] ID:887 | Similarit√©: 0.6401\n",
      "    h: bien et donc vous souhaiteriez les recevoir par courrier\n"
     ]
    }
   ],
   "source": [
    "test_question = \"Quels sont les stages disponibles ?\"\n",
    "\n",
    "print(f\"Question : {test_question}\\n\")\n",
    "\n",
    "result = generate_answer(test_question, top_k=3, verbose=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìù R√âPONSE\")\n",
    "print(\"=\" * 70)\n",
    "print(result[\"answer\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìö SOURCES UTILIS√âES\")\n",
    "print(\"=\" * 70)\n",
    "for i, (_id, text, sim) in enumerate(result[\"contexts\"], 1):\n",
    "    preview = text[:150] + \"...\" if len(text) > 150 else text\n",
    "    print(f\"\\n[{i}] ID:{_id} | Similarit√©: {sim:.4f}\")\n",
    "    print(f\"    {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acb35d0",
   "metadata": {},
   "source": [
    "Mode interactif - Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f734db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Mode interactif disponible\n",
      "  Usage: chat_interactive(top_k=3)\n"
     ]
    }
   ],
   "source": [
    "def chat_interactive(top_k: int = 3, chat_model: str = CHAT_MODEL):\n",
    "    \"\"\"\n",
    "    Lance une session de chat interactive\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ü§ñ CHATBOT RAG INTERACTIF\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Mod√®le : {chat_model}\")\n",
    "    print(f\"Top-K : {top_k} documents\")\n",
    "    print(\"\\nTapez 'exit' ou 'quit' pour quitter\")\n",
    "    print(\"Tapez 'debug' pour voir le dernier prompt\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    last_result = None\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"üí¨ Vous: \").strip()\n",
    "            \n",
    "            if not question:\n",
    "                continue\n",
    "            \n",
    "            if question.lower() in ['exit', 'quit', 'q']:\n",
    "                print(\"\\nüëã Au revoir !\")\n",
    "                break\n",
    "            \n",
    "            if question.lower() == 'debug' and last_result:\n",
    "                print(\"\\n\" + \"=\" * 70)\n",
    "                print(\"üîç DERNIER PROMPT\")\n",
    "                print(\"=\" * 70)\n",
    "                print(last_result[\"prompt\"])\n",
    "                print(\"=\" * 70 + \"\\n\")\n",
    "                continue\n",
    "            \n",
    "            # G√©n√©rer la r√©ponse\n",
    "            last_result = generate_answer(question, top_k=top_k, chat_model=chat_model)\n",
    "            \n",
    "            print(f\"\\nü§ñ Assistant: {last_result['answer']}\\n\")\n",
    "            \n",
    "            # Afficher les sources\n",
    "            if last_result[\"contexts\"]:\n",
    "                print(\"üìö Sources:\")\n",
    "                for i, (_id, text, sim) in enumerate(last_result[\"contexts\"], 1):\n",
    "                    preview = text[:80] + \"...\" if len(text) > 80 else text\n",
    "                    print(f\"  [{i}] (score: {sim:.3f}) {preview}\")\n",
    "                print()\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nüëã Au revoir !\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Erreur: {e}\\n\")\n",
    "\n",
    "# D√©commenter pour lancer le chat interactif\n",
    "# chat_interactive()\n",
    "\n",
    "print(\"‚úì Mode interactif disponible\")\n",
    "print(\"  Usage: chat_interactive(top_k=3)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6fd487",
   "metadata": {},
   "source": [
    "Fonction de test par lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84aa02d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Fonction de test par lot disponible\n",
      "  Usage: test_questions(['question1', 'question2'], top_k=3)\n"
     ]
    }
   ],
   "source": [
    "def test_questions(questions: list[str], top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Teste plusieurs questions en une fois\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"üß™ TEST DE {len(questions)} QUESTIONS\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"\\n[{i}/{len(questions)}] Question: {q}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        try:\n",
    "            result = generate_answer(q, top_k=top_k)\n",
    "            print(f\"R√©ponse: {result['answer'][:200]}...\")\n",
    "            print(f\"Sources: {len(result['contexts'])} document(s)\")\n",
    "            results.append({\"question\": q, \"success\": True, \"result\": result})\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur: {e}\")\n",
    "            results.append({\"question\": q, \"success\": False, \"error\": str(e)})\n",
    "    \n",
    "    # R√©sum√©\n",
    "    success_count = sum(1 for r in results if r[\"success\"])\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"‚úì {success_count}/{len(questions)} questions trait√©es avec succ√®s\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Exemple d'utilisation\n",
    "test_questions_list = [\n",
    "    \"Quels sont les stages disponibles ?\",\n",
    "    \"Comment postuler ?\",\n",
    "    \"Quelles sont les conditions d'admission ?\",\n",
    "]\n",
    "\n",
    "# D√©commenter pour tester\n",
    "# results = test_questions(test_questions_list, top_k=3)\n",
    "\n",
    "print(\"‚úì Fonction de test par lot disponible\")\n",
    "print(\"  Usage: test_questions(['question1', 'question2'], top_k=3)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
